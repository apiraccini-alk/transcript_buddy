from pathlib import Path
import io
import logging
import tiktoken
import docx
import markdown
import re

from utils.llm import llm, prompts


def process_file(raw_file: Path, output_path: Path) -> None:
    """
    Process the raw file to generate a cleaned and formatted transcript.

    Args:
        raw_file (Path): The path to the raw input file.
        output_path (Path): The path to the directory where the output file will be saved.
    """
    # Parse raw file
    raw_text = parse_raw_document(raw_file)

    # Obtain text chunks
    text_chunks = chunk_text(raw_text)

    # Clean text chunks
    cleaned_chunks = []
    for chunk in text_chunks:
        chunk_summary = llm(
            system_prompt=prompts["CLEANER"],
            user_text=f"This is the transcript chunk: {chunk}",
            oai=True,
        )
        cleaned_chunks.append(chunk_summary)
    cleaned_text = " ".join(cleaned_chunks)

    # Obtain the final summary
    final_summary = llm(system_prompt=prompts["FORMATTER"], user_text=cleaned_text, oai=True)
    
    # Add header and disclaimer
    header = f"*Amplifon CoE - Alkemy - onboarding sessions*\n\n---\n\n"
    disclaimer = f"\n\n---\n\n*This text was generated by an AI language model and may contain errors or inaccuracies.*"
    final_summary = header + final_summary + disclaimer

    # Save output
    formatted_filename = f"{format_filename(raw_file.stem)}___ai_processed_transcript.html"
    with open(output_path / formatted_filename, "w") as f:
        f.write(markdown.markdown(final_summary))
    logging.info(
        f"Processed file: {raw_file} -> {output_path / formatted_filename}"
    )


def format_filename(filename: str) -> str:
    filename = filename.lower()
    filename = re.sub(r'[^a-z0-9\s]', '', filename)
    filename = re.sub(r'\s+', ' ', filename).strip()
    return filename.replace(' ', '_')


def parse_raw_document(raw_file: Path) -> str:
    """
    Parse the raw document to extract text.

    Args:
        raw_file (Path): The path to the raw input file.

    Returns:
        str: The extracted text from the document.
    """
    logging.info(f"Raw file: {raw_file}")

    with io.open(raw_file, "rb") as f:
        content = f.read()
    doc = docx.Document(io.BytesIO(content))

    text = ""
    for para in doc.paragraphs:
        text += para.text + "\n"

    return text


def chunk_text(text: str, chunk_size: int = 6000, overlap: int = 0) -> list[str]:
    """
    Chunk the text into smaller segments based on token count.

    Args:
        text (str): The input text to be chunked.
        chunk_size (int, optional): The size of each chunk in tokens. Defaults to 6000.
        overlap (int, optional): The number of overlapping tokens between chunks. Defaults to 0.

    Returns:
        list[str]: A list of text chunks.
    """
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    chunks = []
    start = 0
    while start < len(tokens):
        end = start + chunk_size
        chunk = tokens[start:end]
        chunks.append(tokenizer.decode(chunk))
        start += chunk_size - overlap
    return chunks
